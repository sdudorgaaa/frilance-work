from bs4 import BeautifulSoup
import requests
from time import sleep
from fake_useragent import UserAgent
'''
delay = 10

url_r = 'https://api.cian.ru/valuation-offer-history/v4/get-house-offer-history-desktop/'

UserAgent().chrome

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36"
}

page0 = requests.post(url_r, timeout=300, headers=headers)


sleep(delay)

soup = BeautifulSoup(page0.text, "lxml")


print(soup)



		f = driver.find_elements(By.CLASS_NAME, 'a10a3f92e9--button--Cp1dl.a10a3f92e9--link-button--Pewgf.a10a3f92e9--XS--Q3OqJ.a10a3f92e9--button--jfWOF')

		sleep(delay)
		for num in f:
			c = num.get_attribute("href")
			if 'sale' in c:
				link = c
		'''


print(range(1.1, 1.4))

Value.GE[3].E[0]
Value.GE[3].E[0]



/html/body/div[3]/div[1]/div[2]/div/div/div[4]/div/div/div[1]/div/div[4]/div[2]/div/div[1]/div[1]/div[1]/div[2]/div[1]/span[2]

/html/body/div[3]/div[1]/div[2]/div/div/div[4]/div/div/div[1]/div/div[3]/div[2]/div/div[1]/div[1]/div[1]/div[2]/div[1]/span[2]

